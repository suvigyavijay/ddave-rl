[COMMON]
SEED = 42

[PPO]
# # Algorithm specific arguments
# total_timesteps: int = 10000000
# """total timesteps of the experiments"""
# learning_rate: float = 1e-3
# """the learning rate of the optimizer"""
# num_envs: int = 64
# """the number of parallel game environments"""
# num_steps: int = 1000
# """the number of steps to run in each environment per policy rollout"""
# anneal_lr: bool = True
# """Toggle learning rate annealing for policy and value networks"""
# gamma: float = 0.99
# """the discount factor gamma"""
# gae_lambda: float = 0.95
# """the lambda for the general advantage estimation"""
# num_minibatches: int = 4
# """the number of mini-batches"""
# update_epochs: int = 4
# """the K epochs to update the policy"""
# norm_adv: bool = True
# """Toggles advantages normalization"""
# clip_coef: float = 0.1
# """the surrogate clipping coefficient"""
# clip_vloss: bool = True
# """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
# ent_coef: float = 0.01
# """coefficient of the entropy"""
# vf_coef: float = 0.5
# """coefficient of the value function"""
# max_grad_norm: float = 0.5
# """the maximum norm for the gradient clipping"""
# target_kl: float = None
# """the target KL divergence threshold"""
TOTAL_TIMESTEPS = 10000000
LEARNING_RATE = 1e-3
NUM_ENVS = 64
NUM_STEPS = 1000
ANNEAL_LR = True
GAMMA = 0.99
GAE_LAMBDA = 0.95
NUM_MINIBATCHES = 4
UPDATE_EPOCHS = 4
NORM_ADV = True
CLIP_COEF = 0.1
CLIP_VLOSS = True
ENT_COEF = 0.01
VF_COEF = 0.5
MAX_GRAD_NORM = 0.5
TARGET_KL = None
